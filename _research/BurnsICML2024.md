---
published: true
author: Tom Burns
show_date: true
last_modified_at: 2024-08-11T00:00:00+01:00

title: "Semantically-correlated memories in a dense associative model"
excerpt: "Mixtures of auto- and hetero-associative memory can perform a range of tasks and suggests new neuroscience-inspired AI interpretability approaches."

permalink: /BurnsICML2024/
collection: research
search: true

layout: single
entries_layout: list # list (default), grid

header:
  teaser: /assets/images/research/BurnsICML2024/multi-scale_tutte-th.jpg
  image: /assets/images/research/BurnsICML2024/multi-scale_tutte.png
  overlay_image: /assets/images/research/BurnsICML2024/multi-scale_tutte.png
  overlay_filter: 0.3 # same as adding an opacity of 0.5 to a black background
  actions:
    - label: "PDF"
      url: "https://proceedings.mlr.press/v235/burns24a.html"
	- label: "GitHub repo"
      url: "https://github.com/tfburns/CDAM"
	- label: "X thread"
      url: "https://x.com/tfburns/status/1778232751348388133"
	- label: "Talk"
      url: "https://www.youtube.com/watch?v=2YqN2hJExSY"

read_time: true
words_per_minute: 200

author_profile: true
share: true
comments: false
related: true

categories:
  - Transformers & associative memory
---

Systems like ChatGPT use powerful AI models called Transformers behind-the-scenes. A key component of Transformers closely resembles a modern version of a neuroscience theory called associative memory. We experience a range of related phenomena in our daily lives, which can be classified into two kinds: auto-association and hetero-association.

Auto-association is when we use partial information to retrieve a full memory, e.g., remembering someone’s birthday party by being reminded of the venue it was held in. Hetero-association is when information of one memory directs us to another memory, e.g., remembering someone’s previous birthday party after remembering their most recent birthday party. In theoretical and computational neuroscience, these phenomena have often been studied separately. In this work, I study them in combination, with an aim of contributing to our theoretical understanding of Transformers and systems like ChatGPT.

I show auto- and hetero-associative mixtures can perform a range of tasks, including:
- simulating a basic model of computation (a finite automaton, as illustrated in the animation below)
- multi-scale representations of networks, e.g., social networks and their groupings at different scales
- stabilizing video memory by preventing the video getting 'stuck' on a frame or skipping forwards or backwards
- providing a more graceful trade-off between memory fidelity and storage capacity
- replicating neuroscience data from an experiment in monkeys

This leads me to suggest neuroscience-inspired analyses for interpreting Transformers, based on the network’s dynamics exhibited during these behaviours. I also pose a new hypothesis for why a phenomenon called ‘superposition’ should be related to ‘context switching’ and ‘data dependent geometry’.

This work was made possible by support of the Institute for Computational & Experimental Research in Mathematics (ICERM) and funding via the National Science Foundation (NSF). Thank you to many for discussions, as well as participants who attended the ‘Physics of Life’ symposium in November 2023.